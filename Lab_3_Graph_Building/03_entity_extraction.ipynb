{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Entity Extraction Basics\n",
    "\n",
    "This notebook demonstrates how to use an LLM to extract structured entities and relationships from text, building a knowledge graph.\n",
    "\n",
    "**Background:** The previous notebooks (01_01, 01_02) taught manual approaches to chunking and embeddings. This notebook uses `SimpleKGPipeline` which handles everything in one step—splitting, embedding, entity extraction, and storage.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between lexical graphs and semantic graphs\n",
    "- Define a schema with entity and relationship types\n",
    "- Use `SimpleKGPipeline` to extract entities from text\n",
    "- Query the combined graph (chunks + entities)\n",
    "\n",
    "---\n",
    "\n",
    "## Lexical vs Semantic Graphs\n",
    "\n",
    "A **lexical graph** represents document structure:\n",
    "```\n",
    "(:Document) <-[:FROM_DOCUMENT]- (:Chunk) -[:NEXT_CHUNK]-> (:Chunk)\n",
    "```\n",
    "\n",
    "A **semantic graph** captures extracted entities and their relationships:\n",
    "```\n",
    "(:Chunk)-[:FROM_CHUNK]->(:Company)-[:OFFERS_PRODUCT]->(:Product)\n",
    "                      \\-[:OFFERS_SERVICE]->(:Service)\n",
    "```\n",
    "\n",
    "The combination enables powerful queries that leverage both content similarity AND structured relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../new-workshops/solutions')\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "\n",
    "from config import Neo4jConfig, get_llm, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data-header",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We use sample SEC 10-K text that contains clear entities (companies, products, executives) for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sample-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text length: 1079 characters\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TEXT = \"\"\"\n",
    "Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, \n",
    "personal computers, tablets, wearables and accessories, and sells a variety of related \n",
    "services. The Company's fiscal year is the 52- or 53-week period that ends on the last \n",
    "Saturday of September.\n",
    "\n",
    "Products\n",
    "\n",
    "iPhone is the Company's line of smartphones based on its iOS operating system. The iPhone \n",
    "product line includes iPhone 14 Pro, iPhone 14, iPhone 13 and iPhone SE. Mac is the Company's \n",
    "line of personal computers based on its macOS operating system. iPad is the Company's line \n",
    "of multi-purpose tablets based on its iPadOS operating system.\n",
    "\n",
    "Services\n",
    "\n",
    "Advertising includes third-party licensing arrangements and the Company's own advertising \n",
    "platforms. AppleCare offers a portfolio of fee-based service and support products. Cloud \n",
    "Services store and keep customers' content up-to-date across all devices. Digital Content \n",
    "operates various platforms for discovering, purchasing, streaming and downloading digital \n",
    "content and apps. Payment Services include Apple Card and Apple Pay.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(f\"Sample text length: {len(SAMPLE_TEXT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j successfully!\n"
     ]
    }
   ],
   "source": [
    "neo4j_config = Neo4jConfig()\n",
    "driver = GraphDatabase.driver(\n",
    "    neo4j_config.uri,\n",
    "    auth=(neo4j_config.username, neo4j_config.password)\n",
    ")\n",
    "driver.verify_connectivity()\n",
    "print(\"Connected to Neo4j successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-header",
   "metadata": {},
   "source": [
    "## Clear Existing Data\n",
    "\n",
    "Clear all nodes from previous runs. `SimpleKGPipeline` will create everything fresh—Documents, Chunks with embeddings, and extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clear-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 2145 nodes\n"
     ]
    }
   ],
   "source": [
    "def clear_graph(driver):\n",
    "    \"\"\"Remove all nodes from previous runs.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            DETACH DELETE n\n",
    "            RETURN count(n) as deleted\n",
    "        \"\"\")\n",
    "        count = result.single()[\"deleted\"]\n",
    "        print(f\"Deleted {count} nodes\")\n",
    "\n",
    "clear_graph(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-header",
   "metadata": {},
   "source": [
    "## Define Schema\n",
    "\n",
    "Define the entity types and relationship types we want to extract. This tells the LLM what to look for.\n",
    "\n",
    "The schema uses simple dictionaries:\n",
    "- **Entity types**: `{\"label\": \"...\", \"description\": \"...\"}`\n",
    "- **Relationship types**: `{\"label\": \"...\", \"description\": \"...\"}`\n",
    "- **Patterns**: Tuples of `(source_entity, relationship, target_entity)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined:\n",
      "  Entity types: ['Company', 'Product', 'Service']\n",
      "  Relationship types: ['OFFERS_PRODUCT', 'OFFERS_SERVICE']\n",
      "  Patterns: [('Company', 'OFFERS_PRODUCT', 'Product'), ('Company', 'OFFERS_SERVICE', 'Service')]\n"
     ]
    }
   ],
   "source": [
    "# Define entity types (can be simple strings or dicts with more detail)\n",
    "ENTITY_TYPES = [\n",
    "    {\"label\": \"Company\", \"description\": \"A company or organization\"},\n",
    "    {\"label\": \"Product\", \"description\": \"A product offered by a company\"},\n",
    "    {\"label\": \"Service\", \"description\": \"A service offered by a company\"},\n",
    "]\n",
    "\n",
    "# Define relationship types\n",
    "RELATIONSHIP_TYPES = [\n",
    "    {\"label\": \"OFFERS_PRODUCT\", \"description\": \"Company offers a product\"},\n",
    "    {\"label\": \"OFFERS_SERVICE\", \"description\": \"Company offers a service\"},\n",
    "]\n",
    "\n",
    "# Define valid patterns (source, relationship, target)\n",
    "PATTERNS = [\n",
    "    (\"Company\", \"OFFERS_PRODUCT\", \"Product\"),\n",
    "    (\"Company\", \"OFFERS_SERVICE\", \"Service\"),\n",
    "]\n",
    "\n",
    "print(\"Schema defined:\")\n",
    "print(f\"  Entity types: {[e['label'] for e in ENTITY_TYPES]}\")\n",
    "print(f\"  Relationship types: {[r['label'] for r in RELATIONSHIP_TYPES]}\")\n",
    "print(f\"  Patterns: {PATTERNS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": [
    "## Initialize LLM and Embedder\n",
    "\n",
    "The pipeline needs:\n",
    "- **LLM** - To extract entities from text\n",
    "- **Embedder** - To generate chunk embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "llm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AzureCliCredential.get_token failed: Please run 'az login' to set up an account\n",
      "DefaultAzureCredential failed to retrieve a token from the included credentials.\n",
      "Attempted credentials:\n",
      "\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n",
      "Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n",
      "\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'client_id', 'token_file_path'.\n",
      "\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n",
      "\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n",
      "\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n",
      "\tAzureCliCredential: Please run 'az login' to set up an account\n",
      "\tAzurePowerShellCredential: PowerShell is not installed\n",
      "\tAzureDeveloperCliCredential: Please run 'azd auth login' from a command prompt to authenticate before using this credential.\n",
      "\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\n",
      "To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Azure authentication failed. Please run:\n  1. az login --use-device-code\n  2. Restart your Jupyter kernel (Kernel → Restart)\n\nOriginal error: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'client_id', 'token_file_path'.\n\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n\tAzureCliCredential: Please run 'az login' to set up an account\n\tAzurePowerShellCredential: PowerShell is not installed\n\tAzureDeveloperCliCredential: Please run 'azd auth login' from a command prompt to authenticate before using this credential.\n\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientAuthenticationError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hands-on-lab-neo4j-and-azure/Lab_3_Graph_Building/../new-workshops/solutions/config.py:97\u001b[39m, in \u001b[36m_get_azure_token\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     96\u001b[39m credential = DefaultAzureCredential()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m token = \u001b[43mcredential\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m token.token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hands-on-lab-neo4j-and-azure/.venv/lib/python3.12/site-packages/azure/identity/_credentials/default.py:344\u001b[39m, in \u001b[36mDefaultAzureCredential.get_token\u001b[39m\u001b[34m(self, claims, tenant_id, *scopes, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     token = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hands-on-lab-neo4j-and-azure/.venv/lib/python3.12/site-packages/azure/identity/_credentials/chained.py:159\u001b[39m, in \u001b[36mChainedTokenCredential.get_token\u001b[39m\u001b[34m(self, claims, tenant_id, enable_cae, *scopes, **kwargs)\u001b[39m\n\u001b[32m    158\u001b[39m _LOGGER.warning(message)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientAuthenticationError(message=message)\n",
      "\u001b[31mClientAuthenticationError\u001b[39m: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'client_id', 'token_file_path'.\n\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n\tAzureCliCredential: Please run 'az login' to set up an account\n\tAzurePowerShellCredential: PowerShell is not installed\n\tAzureDeveloperCliCredential: Please run 'azd auth login' from a command prompt to authenticate before using this credential.\n\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm = \u001b[43mget_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m embedder = get_embedder()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hands-on-lab-neo4j-and-azure/Lab_3_Graph_Building/../new-workshops/solutions/config.py:131\u001b[39m, in \u001b[36mget_llm\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03mGet LLM using Microsoft Foundry's OpenAI-compatible endpoint.\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[33;03mUses Azure CLI credentials to authenticate with the inference endpoint.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m config = get_agent_config()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m token = \u001b[43m_get_azure_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m OpenAILLM(\n\u001b[32m    134\u001b[39m     model_name=config.model_name,\n\u001b[32m    135\u001b[39m     base_url=config.inference_endpoint,\n\u001b[32m    136\u001b[39m     api_key=token,\n\u001b[32m    137\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/hands-on-lab-neo4j-and-azure/Lab_3_Graph_Building/../new-workshops/solutions/config.py:100\u001b[39m, in \u001b[36m_get_azure_token\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token.token\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    101\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAzure authentication failed. Please run:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  1. az login --use-device-code\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  2. Restart your Jupyter kernel (Kernel → Restart)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Azure authentication failed. Please run:\n  1. az login --use-device-code\n  2. Restart your Jupyter kernel (Kernel → Restart)\n\nOriginal error: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n\tEnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\nVisit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\n\tWorkloadIdentityCredential: WorkloadIdentityCredential authentication unavailable. The workload options are not fully configured. See the troubleshooting guide for more information: https://aka.ms/azsdk/python/identity/workloadidentitycredential/troubleshoot. Missing required arguments: 'client_id', 'token_file_path'.\n\tManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n\tSharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n\tVisualStudioCodeCredential: VisualStudioCodeCredential requires the 'azure-identity-broker' package to be installed. You must also ensure you have the Azure Resources extension installed and have signed in to Azure via Visual Studio Code.\n\tAzureCliCredential: Please run 'az login' to set up an account\n\tAzurePowerShellCredential: PowerShell is not installed\n\tAzureDeveloperCliCredential: Please run 'azd auth login' from a command prompt to authenticate before using this credential.\n\tBrokerCredential: InteractiveBrowserBrokerCredential unavailable. The 'azure-identity-broker' package is required to use brokered authentication.\nTo mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot."
     ]
    }
   ],
   "source": [
    "llm = get_llm()\n",
    "embedder = get_embedder()\n",
    "\n",
    "print(f\"LLM: {llm.model_name}\")\n",
    "print(f\"Embedder: {embedder.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-header",
   "metadata": {},
   "source": [
    "## Create SimpleKGPipeline\n",
    "\n",
    "The `SimpleKGPipeline` combines all the steps we've learned:\n",
    "1. Text splitting\n",
    "2. Embedding generation\n",
    "3. Entity extraction (via LLM)\n",
    "4. Writing to Neo4j\n",
    "\n",
    "It handles everything automatically based on the schema you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SimpleKGPipeline(\n",
    "    llm=llm,\n",
    "    driver=driver,\n",
    "    embedder=embedder,\n",
    "    schema={\n",
    "        \"node_types\": ENTITY_TYPES,\n",
    "        \"relationship_types\": RELATIONSHIP_TYPES,\n",
    "        \"patterns\": PATTERNS,\n",
    "    },\n",
    "    from_pdf=False,  # We're using text, not PDF\n",
    "    on_error=\"IGNORE\",  # Skip errors for demo\n",
    ")\n",
    "\n",
    "print(\"SimpleKGPipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## Run Entity Extraction\n",
    "\n",
    "Run the pipeline on our sample text. This will:\n",
    "1. Split text into chunks\n",
    "2. Generate embeddings for each chunk\n",
    "3. Use the LLM to extract entities and relationships\n",
    "4. Write everything to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running entity extraction...\")\n",
    "print(\"(This may take 30-60 seconds)\\n\")\n",
    "\n",
    "result = await pipeline.run_async(text=SAMPLE_TEXT)\n",
    "\n",
    "print(\"Entity extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## Explore Extracted Entities\n",
    "\n",
    "Query the graph to see what entities were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-entities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entities(driver):\n",
    "    \"\"\"Display extracted entities by type.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Get entity counts by label\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE n:Company OR n:Product OR n:Service\n",
    "            RETURN labels(n)[0] as label, count(n) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"=== Entity Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['label']}: {record['count']}\")\n",
    "        \n",
    "        # List entities by type\n",
    "        for label in [\"Company\", \"Product\", \"Service\"]:\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (n:{label})\n",
    "                RETURN n.name as name\n",
    "                ORDER BY n.name\n",
    "                LIMIT 10\n",
    "            \"\"\")\n",
    "            names = [record[\"name\"] for record in result]\n",
    "            if names:\n",
    "                print(f\"\\n=== {label} Entities ===\")\n",
    "                for name in names:\n",
    "                    print(f\"  - {name}\")\n",
    "\n",
    "show_entities(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relationships-header",
   "metadata": {},
   "source": [
    "## Explore Relationships\n",
    "\n",
    "See the relationships between extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-relationships",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_relationships(driver):\n",
    "    \"\"\"Display extracted relationships.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (c:Company)-[r]->(target)\n",
    "            WHERE type(r) IN ['OFFERS_PRODUCT', 'OFFERS_SERVICE']\n",
    "            RETURN c.name as company, type(r) as relationship, \n",
    "                   labels(target)[0] as target_type, target.name as target_name\n",
    "            ORDER BY c.name, type(r)\n",
    "            LIMIT 20\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"=== Extracted Relationships ===\")\n",
    "        for record in result:\n",
    "            print(f\"  ({record['company']}) -[{record['relationship']}]-> ({record['target_name']})\")\n",
    "\n",
    "\n",
    "show_relationships(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-header",
   "metadata": {},
   "source": [
    "## Query the Combined Graph\n",
    "\n",
    "Now we can query both the lexical graph (Documents, Chunks) and semantic graph (Entities) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_summary(driver):\n",
    "    \"\"\"Show a summary of the complete graph.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Count all node types\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            UNWIND labels(n) as label\n",
    "            RETURN label, count(*) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"=== Node Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['label']}: {record['count']}\")\n",
    "        \n",
    "        # Count relationship types\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH ()-[r]->()\n",
    "            RETURN type(r) as type, count(*) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"\\n=== Relationship Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['type']}: {record['count']}\")\n",
    "\n",
    "show_graph_summary(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-entity-header",
   "metadata": {},
   "source": [
    "## Find Chunks Containing Entities\n",
    "\n",
    "Query chunks that are connected to specific entities via `FROM_CHUNK` relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks_for_entity(driver, entity_name: str):\n",
    "    \"\"\"Find chunks that mention a specific entity.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Entities point TO chunks via FROM_CHUNK\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e)-[:FROM_CHUNK]->(c:Chunk)\n",
    "            WHERE e.name CONTAINS $name\n",
    "            RETURN e.name as entity, labels(e)[0] as type, c.text as chunk_text\n",
    "            LIMIT 5\n",
    "        \"\"\", name=entity_name)\n",
    "        \n",
    "        records = list(result)\n",
    "        if records:\n",
    "            print(f\"Chunks mentioning '{entity_name}':\")\n",
    "            for record in records:\n",
    "                print(f\"\\n  Entity: {record['entity']} ({record['type']})\")\n",
    "                print(f\"  Chunk: {record['chunk_text']}\")\n",
    "        else:\n",
    "            print(f\"No chunks found mentioning '{entity_name}'\")\n",
    "\n",
    "# Try finding chunks for a product\n",
    "find_chunks_for_entity(driver, \"iPhone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Lexical vs Semantic graphs** - Documents/Chunks vs Entities/Relationships\n",
    "2. **Schema definition** - Telling the LLM what entities and relationships to extract\n",
    "3. **SimpleKGPipeline** - Automated pipeline for text → knowledge graph\n",
    "4. **Combined queries** - Leveraging both document structure and extracted entities\n",
    "\n",
    "You now have a complete knowledge graph with:\n",
    "- **Documents** - Source file metadata\n",
    "- **Chunks** - Text segments with embeddings\n",
    "- **Entities** - Extracted companies, products, services\n",
    "- **Relationships** - Connections between entities\n",
    "\n",
    "Note that we've been working with a small sample of text. In the next notebook, you'll load the full dataset to see much richer results.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Working with the Full Dataset](01_04_full_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "driver.close()\n",
    "print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-azure-ai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
