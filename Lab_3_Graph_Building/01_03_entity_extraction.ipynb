{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Entity Extraction Basics\n",
    "\n",
    "This notebook demonstrates how to use an LLM to extract structured entities and relationships from text, building a knowledge graph.\n",
    "\n",
    "**Background:** The previous notebooks (01_01, 01_02) taught manual approaches to chunking and embeddings. This notebook uses `SimpleKGPipeline` which handles everything in one step—splitting, embedding, entity extraction, and storage.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between lexical graphs and semantic graphs\n",
    "- Define a schema with entity and relationship types\n",
    "- Use `SimpleKGPipeline` to extract entities from text\n",
    "- Query the combined graph (chunks + entities)\n",
    "\n",
    "---\n",
    "\n",
    "## Lexical vs Semantic Graphs\n",
    "\n",
    "A **lexical graph** represents document structure:\n",
    "```\n",
    "(:Document) <-[:FROM_DOCUMENT]- (:Chunk) -[:NEXT_CHUNK]-> (:Chunk)\n",
    "```\n",
    "\n",
    "A **semantic graph** captures extracted entities and their relationships:\n",
    "```\n",
    "(:Chunk)-[:FROM_CHUNK]->(:Company)-[:OFFERS_PRODUCT]->(:Product)\n",
    "                      \\-[:OFFERS_SERVICE]->(:Service)\n",
    "```\n",
    "\n",
    "The combination enables powerful queries that leverage both content similarity AND structured relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../new-workshops/solutions')\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "\n",
    "from config import Neo4jConfig, get_llm, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data-header",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We use sample SEC 10-K text that contains clear entities (companies, products, executives) for extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = \"\"\"\n",
    "Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, \n",
    "personal computers, tablets, wearables and accessories, and sells a variety of related \n",
    "services. The Company's fiscal year is the 52- or 53-week period that ends on the last \n",
    "Saturday of September.\n",
    "\n",
    "Products\n",
    "\n",
    "iPhone is the Company's line of smartphones based on its iOS operating system. The iPhone \n",
    "product line includes iPhone 14 Pro, iPhone 14, iPhone 13 and iPhone SE. Mac is the Company's \n",
    "line of personal computers based on its macOS operating system. iPad is the Company's line \n",
    "of multi-purpose tablets based on its iPadOS operating system.\n",
    "\n",
    "Services\n",
    "\n",
    "Advertising includes third-party licensing arrangements and the Company's own advertising \n",
    "platforms. AppleCare offers a portfolio of fee-based service and support products. Cloud \n",
    "Services store and keep customers' content up-to-date across all devices. Digital Content \n",
    "operates various platforms for discovering, purchasing, streaming and downloading digital \n",
    "content and apps. Payment Services include Apple Card and Apple Pay.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(f\"Sample text length: {len(SAMPLE_TEXT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_config = Neo4jConfig()\n",
    "driver = GraphDatabase.driver(\n",
    "    neo4j_config.uri,\n",
    "    auth=(neo4j_config.username, neo4j_config.password)\n",
    ")\n",
    "driver.verify_connectivity()\n",
    "print(\"Connected to Neo4j successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-header",
   "metadata": {},
   "source": [
    "## Clear Existing Data\n",
    "\n",
    "Clear all nodes from previous runs. `SimpleKGPipeline` will create everything fresh—Documents, Chunks with embeddings, and extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_graph(driver):\n",
    "    \"\"\"Remove all nodes from previous runs.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            DETACH DELETE n\n",
    "            RETURN count(n) as deleted\n",
    "        \"\"\")\n",
    "        count = result.single()[\"deleted\"]\n",
    "        print(f\"Deleted {count} nodes\")\n",
    "\n",
    "clear_graph(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-header",
   "metadata": {},
   "source": [
    "## Define Schema\n",
    "\n",
    "Define the entity types and relationship types we want to extract. This tells the LLM what to look for.\n",
    "\n",
    "The schema uses simple dictionaries:\n",
    "- **Entity types**: `{\"label\": \"...\", \"description\": \"...\"}`\n",
    "- **Relationship types**: `{\"label\": \"...\", \"description\": \"...\"}`\n",
    "- **Patterns**: Tuples of `(source_entity, relationship, target_entity)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity types (can be simple strings or dicts with more detail)\n",
    "ENTITY_TYPES = [\n",
    "    {\"label\": \"Company\", \"description\": \"A company or organization\"},\n",
    "    {\"label\": \"Product\", \"description\": \"A product offered by a company\"},\n",
    "    {\"label\": \"Service\", \"description\": \"A service offered by a company\"},\n",
    "]\n",
    "\n",
    "# Define relationship types\n",
    "RELATIONSHIP_TYPES = [\n",
    "    {\"label\": \"OFFERS_PRODUCT\", \"description\": \"Company offers a product\"},\n",
    "    {\"label\": \"OFFERS_SERVICE\", \"description\": \"Company offers a service\"},\n",
    "]\n",
    "\n",
    "# Define valid patterns (source, relationship, target)\n",
    "PATTERNS = [\n",
    "    (\"Company\", \"OFFERS_PRODUCT\", \"Product\"),\n",
    "    (\"Company\", \"OFFERS_SERVICE\", \"Service\"),\n",
    "]\n",
    "\n",
    "print(\"Schema defined:\")\n",
    "print(f\"  Entity types: {[e['label'] for e in ENTITY_TYPES]}\")\n",
    "print(f\"  Relationship types: {[r['label'] for r in RELATIONSHIP_TYPES]}\")\n",
    "print(f\"  Patterns: {PATTERNS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": [
    "## Initialize LLM and Embedder\n",
    "\n",
    "The pipeline needs:\n",
    "- **LLM** - To extract entities from text\n",
    "- **Embedder** - To generate chunk embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm()\n",
    "embedder = get_embedder()\n",
    "\n",
    "print(f\"LLM: {llm.model_name}\")\n",
    "print(f\"Embedder: {embedder.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-header",
   "metadata": {},
   "source": [
    "## Create SimpleKGPipeline\n",
    "\n",
    "The `SimpleKGPipeline` combines all the steps we've learned:\n",
    "1. Text splitting\n",
    "2. Embedding generation\n",
    "3. Entity extraction (via LLM)\n",
    "4. Writing to Neo4j\n",
    "\n",
    "It handles everything automatically based on the schema you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SimpleKGPipeline(\n",
    "    llm=llm,\n",
    "    driver=driver,\n",
    "    embedder=embedder,\n",
    "    schema={\n",
    "        \"node_types\": ENTITY_TYPES,\n",
    "        \"relationship_types\": RELATIONSHIP_TYPES,\n",
    "        \"patterns\": PATTERNS,\n",
    "    },\n",
    "    from_pdf=False,  # We're using text, not PDF\n",
    "    on_error=\"IGNORE\",  # Skip errors for demo\n",
    ")\n",
    "\n",
    "print(\"SimpleKGPipeline created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## Run Entity Extraction\n",
    "\n",
    "Run the pipeline on our sample text. This will:\n",
    "1. Split text into chunks\n",
    "2. Generate embeddings for each chunk\n",
    "3. Use the LLM to extract entities and relationships\n",
    "4. Write everything to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running entity extraction...\")\n",
    "print(\"(This may take 30-60 seconds)\\n\")\n",
    "\n",
    "result = await pipeline.run_async(text=SAMPLE_TEXT)\n",
    "\n",
    "print(\"Entity extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## Explore Extracted Entities\n",
    "\n",
    "Query the graph to see what entities were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-entities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_entities(driver):\n",
    "    \"\"\"Display extracted entities by type.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Get entity counts by label\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE n:Company OR n:Product OR n:Service\n",
    "            RETURN labels(n)[0] as label, count(n) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"=== Entity Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['label']}: {record['count']}\")\n",
    "        \n",
    "        # List entities by type\n",
    "        for label in [\"Company\", \"Product\", \"Service\"]:\n",
    "            result = session.run(f\"\"\"\n",
    "                MATCH (n:{label})\n",
    "                RETURN n.name as name\n",
    "                ORDER BY n.name\n",
    "                LIMIT 10\n",
    "            \"\"\")\n",
    "            names = [record[\"name\"] for record in result]\n",
    "            if names:\n",
    "                print(f\"\\n=== {label} Entities ===\")\n",
    "                for name in names:\n",
    "                    print(f\"  - {name}\")\n",
    "\n",
    "show_entities(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relationships-header",
   "metadata": {},
   "source": [
    "## Explore Relationships\n",
    "\n",
    "See the relationships between extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-relationships",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_relationships(driver):\n",
    "    \"\"\"Display extracted relationships.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (c:Company)-[r]->(target)\n",
    "            WHERE type(r) IN ['OFFERS_PRODUCT', 'OFFERS_SERVICE']\n",
    "            RETURN c.name as company, type(r) as relationship, \n",
    "                   labels(target)[0] as target_type, target.name as target_name\n",
    "            ORDER BY c.name, type(r)\n",
    "            LIMIT 20\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"=== Extracted Relationships ===\")\n",
    "        for record in result:\n",
    "            print(f\"  ({record['company']}) -[{record['relationship']}]-> ({record['target_type']}: {record['target_name']})\")\n",
    "\n",
    "show_relationships(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-header",
   "metadata": {},
   "source": [
    "## Query the Combined Graph\n",
    "\n",
    "Now we can query both the lexical graph (Documents, Chunks) and semantic graph (Entities) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_summary(driver):\n",
    "    \"\"\"Show a summary of the complete graph.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Count all node types\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n)\n",
    "            UNWIND labels(n) as label\n",
    "            RETURN label, count(*) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"=== Node Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['label']}: {record['count']}\")\n",
    "        \n",
    "        # Count relationship types\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH ()-[r]->()\n",
    "            RETURN type(r) as type, count(*) as count\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        print(\"\\n=== Relationship Counts ===\")\n",
    "        for record in result:\n",
    "            print(f\"  {record['type']}: {record['count']}\")\n",
    "\n",
    "show_graph_summary(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunk-entity-header",
   "metadata": {},
   "source": [
    "## Find Chunks Containing Entities\n",
    "\n",
    "Query chunks that are connected to specific entities via `FROM_CHUNK` relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks_for_entity(driver, entity_name: str):\n",
    "    \"\"\"Find chunks that mention a specific entity.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Entities point TO chunks via FROM_CHUNK\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e)-[:FROM_CHUNK]->(c:Chunk)\n",
    "            WHERE e.name CONTAINS $name\n",
    "            RETURN e.name as entity, labels(e)[0] as type, c.text as chunk_text\n",
    "            LIMIT 5\n",
    "        \"\"\", name=entity_name)\n",
    "        \n",
    "        records = list(result)\n",
    "        if records:\n",
    "            print(f\"Chunks mentioning '{entity_name}':\")\n",
    "            for record in records:\n",
    "                print(f\"\\n  Entity: {record['entity']} ({record['type']})\")\n",
    "                print(f\"  Chunk: {record['chunk_text']}\")\n",
    "        else:\n",
    "            print(f\"No chunks found mentioning '{entity_name}'\")\n",
    "\n",
    "# Try finding chunks for a product\n",
    "find_chunks_for_entity(driver, \"iPhone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Lexical vs Semantic graphs** - Documents/Chunks vs Entities/Relationships\n",
    "2. **Schema definition** - Telling the LLM what entities and relationships to extract\n",
    "3. **SimpleKGPipeline** - Automated pipeline for text → knowledge graph\n",
    "4. **Combined queries** - Leveraging both document structure and extracted entities\n",
    "\n",
    "You now have a complete knowledge graph with:\n",
    "- **Documents** - Source file metadata\n",
    "- **Chunks** - Text segments with embeddings\n",
    "- **Entities** - Extracted companies, products, services\n",
    "- **Relationships** - Connections between entities\n",
    "\n",
    "Note that we've been working with a small sample of text. In the next notebook, you'll load the full dataset to see much richer results.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Working with the Full Dataset](01_04_full_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "driver.close()\n",
    "print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-azure-ai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
