{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Vector Retriever\n",
    "\n",
    "You will create a vector retriever using the Neo4j GraphRAG Python package with Microsoft Foundry.\n",
    "\n",
    "You will be able to review how the vector index is used to retrieve similar results and how the context can be used by an LLM to provide a response.\n",
    "\n",
    "---\n",
    "\n",
    "Import the required Python modules and set up the Microsoft Foundry configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../new-workshops/solutions')\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "from config import get_neo4j_driver, get_llm, get_embedder, Neo4jConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Create and verify the connection to your Neo4j graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_config = Neo4jConfig()\n",
    "driver = GraphDatabase.driver(\n",
    "    neo4j_config.uri, \n",
    "    auth=(\n",
    "        neo4j_config.username, \n",
    "        neo4j_config.password\n",
    "    ))\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Initialize LLM and Embedder\n",
    "\n",
    "Set up the Large Language Model (LLM) and the embedding model you will use in retrieval-augmented generation (RAG) workflows.\n",
    "\n",
    "- **LLM**: Uses Microsoft Foundry's model via the `OpenAILLM` interface with Azure CLI credentials.\n",
    "- **Embedder**: Uses Microsoft Foundry's embedding API via the `OpenAIEmbeddings` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize LLM and Embedder from Microsoft Foundry ---\n",
    "llm = get_llm()\n",
    "embedder = get_embedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Initialize Vector Retriever\n",
    "\n",
    "Set up the vector-based retriever for semantic search over your Neo4j knowledge graph.\n",
    "\n",
    "> Vector search enables semantic retrieval of text chunks from your Neo4j graph.  \n",
    "> Instead of keyword matching, it finds the most contextually similar passages to your query, even if the wording is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Vector Retriever ---\n",
    "vector_retriever = VectorRetriever(\n",
    "    driver=driver,\n",
    "    index_name='chunkEmbeddings',\n",
    "    embedder=embedder,\n",
    "    return_properties=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The **VectorRetriever** class:\n",
    "- Connects to the Neo4j database using the provided `driver`.\n",
    "- Uses the `chunkEmbeddings` vector index for efficient semantic retrieval.\n",
    "- The `embedder` generates embeddings for the query.\n",
    "- Returns the `text` property from matching chunks.\n",
    "\n",
    "> **Tip:**  \n",
    "> You can modify the `return_properties` list to include additional properties from the retrieved nodes.\n",
    "\n",
    "# Simple Vector Search Diagnostic \n",
    "\n",
    "You can use the vector retriever to search for semantically similar data.\n",
    "\n",
    "Test the vector search by retrieving the top 10 most relevant text chunks from the Neo4j knowledge graph for the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple Vector Search ---\n",
    "query = \"What are the risks that Apple faces?\"\n",
    "result = vector_retriever.search(query_text=query, top_k=10)\n",
    "for item in result.items:\n",
    "    print(f\"Score: {item.metadata['score']:.4f}, Content: {item.content[0:100]}..., id: {item.metadata['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**How it works:**  \n",
    "1. The example `query`, \"What are the risks that Apple faces?\", is created\n",
    "2. `vector_retriever.search()` runs the query and returns the top 10 matches based on vector similarity.\n",
    "3. The results are formatted displaying:\n",
    "    * The similarity score (`Score`)\n",
    "    * A snippet of the retrieved content (`Content`)\n",
    "    * The unique identifier for each chunk (`id`)\n",
    "\n",
    "This diagnostic helps you verify that the vector search is working and inspect the quality of the top results for your query.\n",
    "\n",
    "> **Tip:**\n",
    "> Inspecting the returned results to verify relevance can help you to adjust your chunking or embedding strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Query\n",
    "\n",
    "You can use the `GraphRAG` class to create a retrieval-augmented generation (RAG) pipeline.\n",
    "\n",
    "The `GraphRAG` class combines a Large Language Model (LLM) with a vector-based retriever to answer questions using both semantic search and generative reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize RAG and Perform Search ---\n",
    "query = \"What are the risks that Apple faces?\"\n",
    "rag = GraphRAG(\n",
    "    llm=llm,\n",
    "    retriever=vector_retriever\n",
    ")\n",
    "response = rag.search(query)\n",
    "\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "- **How it works:**  \n",
    "  1. The retriever (`vector_retriever`) finds the most relevant text chunks from the Neo4j graph based on the input query.\n",
    "  2. The LLM (`llm`) uses the retrieved context to generate a natural language answer.\n",
    "  3. The `rag` pipeline is used to `search`.\n",
    "  4. The `answer` in the `response` is printed\n",
    "\n",
    "The `GraphRAG` pipeline provides context-aware, accurate answers grounded in your knowledge graph data.\n",
    "\n",
    "---\n",
    "\n",
    "Experiment with the vector retriever by modifying the `query`, for example:\n",
    "- What products does Microsoft reference?\n",
    "- What warnings have Nvidia given?\n",
    "- What companies mention AI in their filings?\n",
    "\n",
    "[View the complete code](../new-workshops/solutions/02_01_vector_retriever.py)\n",
    "\n",
    "[Move on to the Advanced RAG:Vector Cypher Retriever Notebook](02_02_vector_cypher_retriever.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-azure-ai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
