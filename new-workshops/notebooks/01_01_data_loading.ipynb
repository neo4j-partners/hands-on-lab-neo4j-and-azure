{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Loading Fundamentals\n",
    "\n",
    "This notebook introduces the core concepts of loading text data into Neo4j for GraphRAG applications.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the Document â†’ Chunk graph structure\n",
    "- Connect to Neo4j from a Jupyter notebook\n",
    "- Create Document and Chunk nodes\n",
    "- Link chunks to documents and to each other\n",
    "\n",
    "---\n",
    "\n",
    "## Why Documents and Chunks?\n",
    "\n",
    "When building RAG (Retrieval-Augmented Generation) applications, we split documents into smaller pieces called **chunks**. This is necessary because:\n",
    "\n",
    "1. **Context windows are limited** - LLMs can only process a certain amount of text at once\n",
    "2. **Retrieval precision** - Smaller chunks allow more precise matching to user queries\n",
    "3. **Cost efficiency** - Processing smaller chunks is faster and cheaper\n",
    "\n",
    "The graph structure we'll build:\n",
    "```\n",
    "(:Document) <-[:FROM_DOCUMENT]- (:Chunk) -[:NEXT_CHUNK]-> (:Chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../solutions')\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from config import Neo4jConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll use sample text representing content from an SEC 10-K filing. This text is similar to what you would extract from a real PDF document.\n",
    "\n",
    "> **Note:** In production, you would use `pypdf` or similar libraries to extract text from PDF files. We use pre-defined text here for fast, reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text representing a page from an SEC 10-K filing\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, \n",
    "personal computers, tablets, wearables and accessories, and sells a variety of related \n",
    "services. The Company's fiscal year is the 52- or 53-week period that ends on the last \n",
    "Saturday of September.\n",
    "\n",
    "Products\n",
    "\n",
    "iPhone is the Company's line of smartphones based on its iOS operating system. The iPhone \n",
    "product line includes iPhone 14 Pro, iPhone 14, iPhone 13 and iPhone SE. Mac is the Company's \n",
    "line of personal computers based on its macOS operating system. iPad is the Company's line \n",
    "of multi-purpose tablets based on its iPadOS operating system.\n",
    "\n",
    "Services\n",
    "\n",
    "Advertising includes third-party licensing arrangements and the Company's own advertising \n",
    "platforms. AppleCare offers a portfolio of fee-based service and support products. Cloud \n",
    "Services store and keep customers' content up-to-date across all devices. Digital Content \n",
    "operates various platforms for discovering, purchasing, streaming and downloading digital \n",
    "content and apps. Payment Services include Apple Card and Apple Pay.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Document metadata\n",
    "DOCUMENT_PATH = \"form10k-sample/apple-2023-10k.pdf\"\n",
    "DOCUMENT_PAGE = 1\n",
    "\n",
    "print(f\"Sample text length: {len(SAMPLE_TEXT)} characters\")\n",
    "print(f\"\\n{SAMPLE_TEXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create a connection to your Neo4j database using credentials from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_config = Neo4jConfig()\n",
    "driver = GraphDatabase.driver(\n",
    "    neo4j_config.uri,\n",
    "    auth=(neo4j_config.username, neo4j_config.password)\n",
    ")\n",
    "driver.verify_connectivity()\n",
    "print(\"Connected to Neo4j successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Clear Existing Data (Optional)\n",
    "\n",
    "For a clean start, remove any existing Document and Chunk nodes from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_graph(driver):\n",
    "    \"\"\"Remove all Document and Chunk nodes.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (n) WHERE n:Document OR n:Chunk\n",
    "            DETACH DELETE n\n",
    "            RETURN count(n) as deleted\n",
    "        \"\"\")\n",
    "        count = result.single()[\"deleted\"]\n",
    "        print(f\"Deleted {count} nodes\")\n",
    "\n",
    "clear_graph(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Create Document Node\n",
    "\n",
    "First, create a Document node to represent the source file. This node stores metadata about where the content came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(driver, path: str, page: int) -> str:\n",
    "    \"\"\"Create a Document node and return its element ID.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CREATE (d:Document {path: $path, page: $page})\n",
    "            RETURN elementId(d) as doc_id\n",
    "        \"\"\", path=path, page=page)\n",
    "        return result.single()[\"doc_id\"]\n",
    "\n",
    "doc_id = create_document(driver, DOCUMENT_PATH, DOCUMENT_PAGE)\n",
    "print(f\"Created Document node with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Split Text into Chunks\n",
    "\n",
    "Split the sample text into smaller chunks. For this demonstration, we'll manually split by paragraph.\n",
    "\n",
    "> **Note:** In the next notebook, you'll learn to use `FixedSizeSplitter` for automatic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str) -> list[str]:\n",
    "    \"\"\"Split text into chunks by double newlines (paragraphs).\"\"\"\n",
    "    chunks = [chunk.strip() for chunk in text.split(\"\\n\\n\") if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "chunks = split_into_chunks(SAMPLE_TEXT)\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "    print(f\"  {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Create Chunk Nodes\n",
    "\n",
    "Create Chunk nodes for each piece of text and link them to the Document with `FROM_DOCUMENT` relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(driver, doc_id: str, chunks: list[str]) -> list[str]:\n",
    "    \"\"\"Create Chunk nodes linked to a Document. Returns chunk element IDs.\"\"\"\n",
    "    chunk_ids = []\n",
    "    with driver.session() as session:\n",
    "        for index, text in enumerate(chunks):\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (d:Document) WHERE elementId(d) = $doc_id\n",
    "                CREATE (c:Chunk {text: $text, index: $index})\n",
    "                CREATE (c)-[:FROM_DOCUMENT]->(d)\n",
    "                RETURN elementId(c) as chunk_id\n",
    "            \"\"\", doc_id=doc_id, text=text, index=index)\n",
    "            chunk_id = result.single()[\"chunk_id\"]\n",
    "            chunk_ids.append(chunk_id)\n",
    "            print(f\"Created Chunk {index}\")\n",
    "    return chunk_ids\n",
    "\n",
    "chunk_ids = create_chunks(driver, doc_id, chunks)\n",
    "print(f\"\\nCreated {len(chunk_ids)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Link Chunks with NEXT_CHUNK\n",
    "\n",
    "Create `NEXT_CHUNK` relationships between sequential chunks. This preserves the original document order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_chunks(driver, chunk_ids: list[str]):\n",
    "    \"\"\"Create NEXT_CHUNK relationships between sequential chunks.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        for i in range(len(chunk_ids) - 1):\n",
    "            session.run(\"\"\"\n",
    "                MATCH (c1:Chunk) WHERE elementId(c1) = $id1\n",
    "                MATCH (c2:Chunk) WHERE elementId(c2) = $id2\n",
    "                CREATE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "            \"\"\", id1=chunk_ids[i], id2=chunk_ids[i+1])\n",
    "        print(f\"Created {len(chunk_ids) - 1} NEXT_CHUNK relationships\")\n",
    "\n",
    "link_chunks(driver, chunk_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Verify the Graph Structure\n",
    "\n",
    "Query the graph to see what we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_structure(driver):\n",
    "    \"\"\"Display the Document-Chunk graph structure.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Count nodes\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (d:Document)\n",
    "            OPTIONAL MATCH (d)<-[:FROM_DOCUMENT]-(c:Chunk)\n",
    "            RETURN d.path as document, d.page as page, count(c) as chunks\n",
    "        \"\"\")\n",
    "        print(\"=== Graph Structure ===\")\n",
    "        for record in result:\n",
    "            print(f\"Document: {record['document']} (page {record['page']})\")\n",
    "            print(f\"  Chunks: {record['chunks']}\")\n",
    "        \n",
    "        # Show chunk chain\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            OPTIONAL MATCH (c)-[:NEXT_CHUNK]->(next:Chunk)\n",
    "            RETURN c.index as idx, \n",
    "                   c.text as text,\n",
    "                   next.index as next_idx\n",
    "            ORDER BY c.index\n",
    "        \"\"\")\n",
    "        print(\"\\n=== Chunk Chain ===\")\n",
    "        for record in result:\n",
    "            next_str = f\" -> Chunk {record['next_idx']}\" if record['next_idx'] is not None else \" (end)\"\n",
    "            print(f\"Chunk {record['idx']}: \\\"{record['text']}\\\"{next_str}\")\n",
    "\n",
    "show_graph_structure(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Document-Chunk structure** - Documents are split into chunks for efficient retrieval\n",
    "2. **FROM_DOCUMENT relationship** - Links chunks back to their source document\n",
    "3. **NEXT_CHUNK relationship** - Preserves the sequential order of chunks\n",
    "\n",
    "This basic structure is the foundation for RAG applications. In the next notebooks, you'll learn to:\n",
    "- Add **embeddings** to chunks for semantic search (01_02)\n",
    "- Extract **entities** from chunks to build a knowledge graph (01_03)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Embeddings and Vector Search](01_02_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "driver.close()\n",
    "print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-workshops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
