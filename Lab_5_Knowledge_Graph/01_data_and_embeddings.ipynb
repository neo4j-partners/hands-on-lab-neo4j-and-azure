{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Loading and Embeddings\n",
    "\n",
    "This notebook covers the complete data preparation pipeline for GraphRAG applications: loading text data into Neo4j as a Document-Chunk graph structure, then enriching it with embeddings for semantic search.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the Document → Chunk graph structure\n",
    "- Connect to Neo4j and create Document and Chunk nodes\n",
    "- Link chunks with `FROM_DOCUMENT` and `NEXT_CHUNK` relationships\n",
    "- Understand what embeddings are and why they matter\n",
    "- Generate embeddings using Microsoft Foundry\n",
    "- Create a vector index and perform similarity search\n",
    "\n",
    "---\n",
    "\n",
    "## Why Documents and Chunks?\n",
    "\n",
    "When building GraphRAG applications, we split documents into smaller pieces called **chunks** because:\n",
    "\n",
    "1. **Context windows are limited** - LLMs can only process a certain amount of text at once\n",
    "2. **Retrieval precision** - Smaller chunks allow more precise matching to user queries\n",
    "3. **Cost efficiency** - Processing smaller chunks is faster and cheaper\n",
    "\n",
    "The graph structure we'll build:\n",
    "```\n",
    "(:Document) <-[:FROM_DOCUMENT]- (:Chunk) -[:NEXT_CHUNK]-> (:Chunk)\n",
    "```\n",
    "\n",
    "We'll use `FixedSizeSplitter` from the [neo4j-graphrag-python](https://neo4j.com/docs/neo4j-graphrag-python/current/) library to split text into chunks:\n",
    "\n",
    "- `chunk_size`: Maximum characters per chunk\n",
    "- `chunk_overlap`: Characters shared between consecutive chunks for context continuity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "from data_utils import Neo4jConnection, DataLoader, split_text, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll load text from `company_data.txt` representing content from an SEC 10-K filing.\n",
    "\n",
    "> **Note:** In production, you would use `pypdf` or similar libraries to extract text from PDF files. We use a pre-defined text file here for fast, reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text from file using DataLoader\n",
    "loader = DataLoader(\"company_data.txt\")\n",
    "SAMPLE_TEXT = loader.text\n",
    "\n",
    "# Document metadata\n",
    "DOCUMENT_PATH = \"form10k-sample/apple-2023-10k.pdf\"\n",
    "DOCUMENT_PAGE = 1\n",
    "\n",
    "metadata = loader.get_metadata()\n",
    "print(f\"Loaded from: {metadata['name']}\")\n",
    "print(f\"Sample text length: {metadata['size']} characters\")\n",
    "print(f\"\\n{SAMPLE_TEXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create a connection to your Neo4j database using the `Neo4jConnection` utility class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Clear Existing Data (Optional)\n",
    "\n",
    "For a clean start, remove any existing Document and Chunk nodes from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j.clear_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Building the Document-Chunk Graph\n",
    "\n",
    "First, we'll create the basic graph structure with Document and Chunk nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Create Document Node\n",
    "\n",
    "Create a Document node to represent the source file. This node stores metadata about where the content came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(driver, path: str, page: int) -> str:\n",
    "    \"\"\"Create a Document node and return its element ID.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CREATE (d:Document {path: $path, page: $page})\n",
    "            RETURN elementId(d) as doc_id\n",
    "        \"\"\", path=path, page=page)\n",
    "        return result.single()[\"doc_id\"]\n",
    "\n",
    "doc_id = create_document(driver, DOCUMENT_PATH, DOCUMENT_PAGE)\n",
    "print(f\"Created Document node with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Split Text into Chunks\n",
    "\n",
    "Use `FixedSizeSplitter` from neo4j-graphrag-python to split the text into chunks with configurable size and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text using the utility function\n",
    "chunks = split_text(SAMPLE_TEXT, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "    print(f\"  {chunk[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Create Chunk Nodes\n",
    "\n",
    "Create Chunk nodes for each piece of text and link them to the Document with `FROM_DOCUMENT` relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(driver, doc_id: str, chunks: list[str]) -> list[str]:\n",
    "    \"\"\"Create Chunk nodes linked to a Document. Returns chunk element IDs.\"\"\"\n",
    "    chunk_ids = []\n",
    "    with driver.session() as session:\n",
    "        for index, text in enumerate(chunks):\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (d:Document) WHERE elementId(d) = $doc_id\n",
    "                CREATE (c:Chunk {text: $text, index: $index})\n",
    "                CREATE (c)-[:FROM_DOCUMENT]->(d)\n",
    "                RETURN elementId(c) as chunk_id\n",
    "            \"\"\", doc_id=doc_id, text=text, index=index)\n",
    "            chunk_id = result.single()[\"chunk_id\"]\n",
    "            chunk_ids.append(chunk_id)\n",
    "            print(f\"Created Chunk {index}\")\n",
    "    return chunk_ids\n",
    "\n",
    "chunk_ids = create_chunks(driver, doc_id, chunks)\n",
    "print(f\"\\nCreated {len(chunk_ids)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Link Chunks with NEXT_CHUNK\n",
    "\n",
    "Create `NEXT_CHUNK` relationships between sequential chunks. This preserves the original document order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_chunks(driver, chunk_ids: list[str]):\n",
    "    \"\"\"Create NEXT_CHUNK relationships between sequential chunks.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        for i in range(len(chunk_ids) - 1):\n",
    "            session.run(\"\"\"\n",
    "                MATCH (c1:Chunk) WHERE elementId(c1) = $id1\n",
    "                MATCH (c2:Chunk) WHERE elementId(c2) = $id2\n",
    "                CREATE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "            \"\"\", id1=chunk_ids[i], id2=chunk_ids[i+1])\n",
    "        print(f\"Created {len(chunk_ids) - 1} NEXT_CHUNK relationships\")\n",
    "\n",
    "link_chunks(driver, chunk_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Verify the Graph Structure\n",
    "\n",
    "Query the graph to see what we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_structure(driver):\n",
    "    \"\"\"Display the Document-Chunk graph structure.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Count nodes\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (d:Document)\n",
    "            OPTIONAL MATCH (d)<-[:FROM_DOCUMENT]-(c:Chunk)\n",
    "            RETURN d.path as document, d.page as page, count(c) as chunks\n",
    "        \"\"\")\n",
    "        print(\"=== Graph Structure ===\")\n",
    "        for record in result:\n",
    "            print(f\"Document: {record['document']} (page {record['page']})\")\n",
    "            print(f\"  Chunks: {record['chunks']}\")\n",
    "        \n",
    "        # Show chunk chain\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            OPTIONAL MATCH (c)-[:NEXT_CHUNK]->(next:Chunk)\n",
    "            RETURN c.index as idx, \n",
    "                   substring(c.text, 0, 50) as text,\n",
    "                   next.index as next_idx\n",
    "            ORDER BY c.index\n",
    "        \"\"\")\n",
    "        print(\"\\n=== Chunk Chain ===\")\n",
    "        for record in result:\n",
    "            next_str = f\" -> Chunk {record['next_idx']}\" if record['next_idx'] is not None else \" (end)\"\n",
    "            print(f\"Chunk {record['idx']}: \\\"{record['text']}...\\\"{next_str}\")\n",
    "\n",
    "show_graph_structure(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Adding Embeddings for Semantic Search\n",
    "\n",
    "Now that we have our Document-Chunk graph, we'll add embeddings to enable semantic search. Embeddings are numerical representations (vectors) of text that capture semantic meaning.\n",
    "\n",
    "```\n",
    "\"Apple makes iPhones\" → [0.12, -0.45, 0.78, ...] (1536 dimensions)\n",
    "\"The company produces smartphones\" → [0.11, -0.44, 0.77, ...] (similar vector!)\n",
    "```\n",
    "\n",
    "Similar texts have similar embeddings, enabling **semantic search** - finding content by meaning rather than exact keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Initialize Embedder\n",
    "\n",
    "Create an embedder using Microsoft Foundry. This uses the `text-embedding-ada-002` model which produces 1536-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = get_embedder()\n",
    "print(f\"Embedder initialized: {embedder.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Generate and Store Embeddings\n",
    "\n",
    "Generate an embedding vector for each chunk and update the nodes in Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_to_chunks(driver, embedder, chunk_ids: list[str]):\n",
    "    \"\"\"Generate embeddings for chunks and store them in Neo4j.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        for i, chunk_id in enumerate(chunk_ids):\n",
    "            # Get chunk text\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (c:Chunk) WHERE elementId(c) = $chunk_id\n",
    "                RETURN c.text as text\n",
    "            \"\"\", chunk_id=chunk_id)\n",
    "            text = result.single()[\"text\"]\n",
    "            \n",
    "            # Generate embedding\n",
    "            embedding = embedder.embed_query(text)\n",
    "            \n",
    "            # Store embedding\n",
    "            session.run(\"\"\"\n",
    "                MATCH (c:Chunk) WHERE elementId(c) = $chunk_id\n",
    "                SET c.embedding = $embedding\n",
    "            \"\"\", chunk_id=chunk_id, embedding=embedding)\n",
    "            \n",
    "            print(f\"Chunk {i}: Generated {len(embedding)}-dimensional embedding\")\n",
    "    \n",
    "    print(f\"\\nAdded embeddings to {len(chunk_ids)} chunks\")\n",
    "\n",
    "add_embeddings_to_chunks(driver, embedder, chunk_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Create Vector Index\n",
    "\n",
    "Create a vector index in Neo4j for efficient similarity search. The index uses cosine similarity to compare embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"chunkEmbeddings\"\n",
    "\n",
    "# Drop existing index if it exists\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n",
    "        print(f\"Dropped existing index: {INDEX_NAME}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create new vector index\n",
    "create_vector_index(\n",
    "    driver=driver,\n",
    "    name=INDEX_NAME,\n",
    "    label=\"Chunk\",\n",
    "    embedding_property=\"embedding\",\n",
    "    dimensions=1536,\n",
    "    similarity_fn=\"cosine\"\n",
    ")\n",
    "print(f\"Created vector index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Vector Similarity Search\n",
    "\n",
    "Now we can search for chunks that are semantically similar to a query. The search:\n",
    "1. Converts the query to an embedding\n",
    "2. Finds chunks with similar embedding vectors\n",
    "3. Returns results ranked by similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(driver, embedder, query: str, top_k: int = 3):\n",
    "    \"\"\"Search for chunks similar to the query.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedder.embed_query(query)\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CALL db.index.vector.queryNodes($index_name, $top_k, $embedding)\n",
    "            YIELD node, score\n",
    "            RETURN node.text as text, node.index as idx, score\n",
    "            ORDER BY score DESC\n",
    "        \"\"\", index_name=INDEX_NAME, top_k=top_k, embedding=query_embedding)\n",
    "        \n",
    "        return list(result)\n",
    "\n",
    "# Test search\n",
    "query = \"What products does Apple make?\"\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = vector_search(driver, embedder, query)\n",
    "for i, record in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Score: {record['score']:.4f} (Chunk {record['idx']})\")\n",
    "    print(f\"    {record['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Compare Different Queries\n",
    "\n",
    "Try different queries to see how semantic search finds relevant content even with different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Tell me about iPhone and Mac computers\",\n",
    "    \"What services does the company offer?\",\n",
    "    \"When does the fiscal year end?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 50)\n",
    "    results = vector_search(driver, embedder, query, top_k=1)\n",
    "    if results:\n",
    "        record = results[0]\n",
    "        print(f\"Best match (score: {record['score']:.4f}):\")\n",
    "        print(f\"  {record['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned the complete data preparation pipeline for GraphRAG:\n",
    "\n",
    "**Part 1 - Graph Structure:**\n",
    "1. **Document-Chunk structure** - Documents are split into chunks for efficient retrieval\n",
    "2. **FROM_DOCUMENT relationship** - Links chunks back to their source document\n",
    "3. **NEXT_CHUNK relationship** - Preserves the sequential order of chunks\n",
    "\n",
    "**Part 2 - Embeddings:**\n",
    "4. **Embeddings** - Numerical vectors that capture semantic meaning\n",
    "5. **Vector storage** - Storing embeddings as node properties\n",
    "6. **Vector index** - Enabling efficient similarity search\n",
    "7. **Semantic search** - Finding content by meaning, not just keywords\n",
    "\n",
    "Your knowledge graph is now ready for retrieval! In the next notebook, you'll learn to use **retrievers** to build GraphRAG pipelines that combine vector search with LLM-generated answers.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [GraphRAG Retrievers](02_graphrag_retrievers.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
