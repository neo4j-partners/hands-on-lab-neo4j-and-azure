{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Embeddings and Vector Search\n",
    "\n",
    "This notebook demonstrates how to generate embeddings for text chunks and perform vector similarity search using Neo4j.\n",
    "\n",
    "**Prerequisites:** Complete [01_01 Data Loading](01_01_data_loading.ipynb) first.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what embeddings are and why they matter for GraphRAG\n",
    "- Use `FixedSizeSplitter` to automatically chunk text\n",
    "- Generate embeddings using Microsoft Foundry\n",
    "- Create a vector index in Neo4j\n",
    "- Perform similarity search to find relevant chunks\n",
    "\n",
    "---\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "Embeddings are numerical representations (vectors) of text that capture semantic meaning. Similar texts have similar embeddings, enabling **semantic search** - finding content by meaning rather than exact keywords.\n",
    "\n",
    "```\n",
    "\"Apple makes iPhones\" → [0.12, -0.45, 0.78, ...] (1536 dimensions)\n",
    "\"The company produces smartphones\" → [0.11, -0.44, 0.77, ...] (similar vector!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "from data_utils import Neo4jConnection, DataLoader, split_text, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Load the sample SEC 10-K text from file using `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text from file\n",
    "loader = DataLoader(\"company_data.txt\")\n",
    "SAMPLE_TEXT = loader.text\n",
    "DOCUMENT_PATH = \"form10k-sample/apple-2023-10k.pdf\"\n",
    "\n",
    "metadata = loader.get_metadata()\n",
    "print(f\"Loaded from: {metadata['name']}\")\n",
    "print(f\"Sample text length: {metadata['size']} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create a connection using the `Neo4jConnection` utility class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Clear Existing Data\n",
    "\n",
    "Remove any existing Document and Chunk nodes from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j.clear_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Split Text with FixedSizeSplitter\n",
    "\n",
    "The `FixedSizeSplitter` from neo4j-graphrag automatically splits text into chunks of a specified size with overlap.\n",
    "\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Characters to overlap between chunks (preserves context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text using the utility function (smaller chunks for demo)\n",
    "chunks_text = split_text(SAMPLE_TEXT, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "print(f\"Split into {len(chunks_text)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks_text):\n",
    "    print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "    print(f\"  {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Initialize Embedder\n",
    "\n",
    "Create an embedder using Microsoft Foundry. This uses the `text-embedding-ada-002` model which produces 1536-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = get_embedder()\n",
    "print(f\"Embedder initialized: {embedder.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Generate an embedding vector for each chunk. This calls the Microsoft Foundry embedding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "chunk_embeddings = []\n",
    "for i, text in enumerate(chunks_text):\n",
    "    embedding = embedder.embed_query(text)\n",
    "    chunk_embeddings.append({\n",
    "        \"text\": text,\n",
    "        \"index\": i,\n",
    "        \"embedding\": embedding\n",
    "    })\n",
    "    print(f\"Chunk {i}: Generated {len(embedding)}-dimensional embedding\")\n",
    "\n",
    "print(f\"\\nFirst 5 values of chunk 0's embedding: {chunk_embeddings[0]['embedding'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Store in Neo4j with Embeddings\n",
    "\n",
    "Create Document and Chunk nodes, storing the embedding vector on each Chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_with_embeddings(driver, doc_path: str, chunk_data: list[dict]):\n",
    "    \"\"\"Store Document and Chunk nodes with embeddings.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Create Document\n",
    "        session.run(\"\"\"\n",
    "            CREATE (d:Document {path: $path})\n",
    "        \"\"\", path=doc_path)\n",
    "        print(f\"Created Document: {doc_path}\")\n",
    "        \n",
    "        # Create Chunks with embeddings\n",
    "        for chunk in chunk_data:\n",
    "            session.run(\"\"\"\n",
    "                MATCH (d:Document {path: $path})\n",
    "                CREATE (c:Chunk {\n",
    "                    text: $text,\n",
    "                    index: $index,\n",
    "                    embedding: $embedding\n",
    "                })\n",
    "                CREATE (c)-[:FROM_DOCUMENT]->(d)\n",
    "            \"\"\", path=doc_path, text=chunk[\"text\"], \n",
    "               index=chunk[\"index\"], embedding=chunk[\"embedding\"])\n",
    "        print(f\"Created {len(chunk_data)} Chunk nodes with embeddings\")\n",
    "        \n",
    "        # Create NEXT_CHUNK relationships\n",
    "        session.run(\"\"\"\n",
    "            MATCH (d:Document {path: $path})<-[:FROM_DOCUMENT]-(c:Chunk)\n",
    "            WITH c ORDER BY c.index\n",
    "            WITH collect(c) as chunks\n",
    "            UNWIND range(0, size(chunks)-2) as i\n",
    "            WITH chunks[i] as c1, chunks[i+1] as c2\n",
    "            CREATE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "        \"\"\", path=doc_path)\n",
    "        print(\"Created NEXT_CHUNK relationships\")\n",
    "\n",
    "store_chunks_with_embeddings(driver, DOCUMENT_PATH, chunk_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Create Vector Index\n",
    "\n",
    "Create a vector index in Neo4j for efficient similarity search. The index uses cosine similarity to compare embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"chunkEmbeddings\"\n",
    "\n",
    "# Drop existing index if it exists\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n",
    "        print(f\"Dropped existing index: {INDEX_NAME}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create new vector index\n",
    "create_vector_index(\n",
    "    driver=driver,\n",
    "    name=INDEX_NAME,\n",
    "    label=\"Chunk\",\n",
    "    embedding_property=\"embedding\",\n",
    "    dimensions=1536,\n",
    "    similarity_fn=\"cosine\"\n",
    ")\n",
    "print(f\"Created vector index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Vector Similarity Search\n",
    "\n",
    "Now we can search for chunks that are semantically similar to a query. The search:\n",
    "1. Converts the query to an embedding\n",
    "2. Finds chunks with similar embedding vectors\n",
    "3. Returns results ranked by similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(driver, embedder, query: str, top_k: int = 3):\n",
    "    \"\"\"Search for chunks similar to the query.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedder.embed_query(query)\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CALL db.index.vector.queryNodes($index_name, $top_k, $embedding)\n",
    "            YIELD node, score\n",
    "            RETURN node.text as text, node.index as idx, score\n",
    "            ORDER BY score DESC\n",
    "        \"\"\", index_name=INDEX_NAME, top_k=top_k, embedding=query_embedding)\n",
    "        \n",
    "        return list(result)\n",
    "\n",
    "# Test search\n",
    "query = \"What products does Apple make?\"\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = vector_search(driver, embedder, query)\n",
    "for i, record in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Score: {record['score']:.4f} (Chunk {record['idx']})\")\n",
    "    print(f\"    {record['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Compare Different Queries\n",
    "\n",
    "Try different queries to see how semantic search finds relevant content even with different wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Tell me about iPhone and Mac computers\",\n",
    "    \"What services does the company offer?\",\n",
    "    \"When does the fiscal year end?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 50)\n",
    "    results = vector_search(driver, embedder, query, top_k=1)\n",
    "    if results:\n",
    "        record = results[0]\n",
    "        print(f\"Best match (score: {record['score']:.4f}):\")\n",
    "        print(f\"  {record['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Embeddings** - Numerical vectors that capture semantic meaning\n",
    "2. **FixedSizeSplitter** - Automatic text chunking with overlap\n",
    "3. **Vector storage** - Storing embeddings as node properties\n",
    "4. **Vector index** - Enabling efficient similarity search\n",
    "5. **Semantic search** - Finding content by meaning, not just keywords\n",
    "\n",
    "The chunks now have embeddings that enable semantic retrieval. In the next notebook, you'll learn to extract **entities** from these chunks to build a richer knowledge graph.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Entity Extraction Basics](01_03_entity_extraction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-azure-ai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
