{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Loading Fundamentals\n",
    "\n",
    "This notebook introduces the core concepts of loading text data into Neo4j for GraphRAG applications.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the Document â†’ Chunk graph structure\n",
    "- Connect to Neo4j from a Jupyter notebook\n",
    "- Create Document and Chunk nodes\n",
    "- Link chunks to documents and to each other\n",
    "\n",
    "---\n",
    "\n",
    "## Why Documents and Chunks?\n",
    "\n",
    "When building GraphRAG applications, we split documents into smaller pieces called **chunks** because:\n",
    "\n",
    "1. **Context windows are limited** - LLMs can only process a certain amount of text at once\n",
    "2. **Retrieval precision** - Smaller chunks allow more precise matching to user queries\n",
    "3. **Cost efficiency** - Processing smaller chunks is faster and cheaper\n",
    "\n",
    "The graph structure we'll build:\n",
    "```\n",
    "(:Document) <-[:FROM_DOCUMENT]- (:Chunk) -[:NEXT_CHUNK]-> (:Chunk)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Text Splitting with neo4j-graphrag-python\n",
    "\n",
    "We'll use `FixedSizeSplitter` from the [neo4j-graphrag-python](https://neo4j.com/docs/neo4j-graphrag-python/current/) library to split text into chunks:\n",
    "\n",
    "- `chunk_size`: Maximum characters per chunk\n",
    "- `chunk_overlap`: Characters shared between consecutive chunks for context continuity\n",
    "- `approximate=True` (default): Avoids splitting words mid-token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import Neo4jConnection, DataLoader, split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll load text from `company_data.txt` representing content from an SEC 10-K filing.\n",
    "\n",
    "> **Note:** In production, you would use `pypdf` or similar libraries to extract text from PDF files. We use a pre-defined text file here for fast, reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text from file using DataLoader\n",
    "loader = DataLoader(\"company_data.txt\")\n",
    "SAMPLE_TEXT = loader.text\n",
    "\n",
    "# Document metadata\n",
    "DOCUMENT_PATH = \"form10k-sample/apple-2023-10k.pdf\"\n",
    "DOCUMENT_PAGE = 1\n",
    "\n",
    "metadata = loader.get_metadata()\n",
    "print(f\"Loaded from: {metadata['name']}\")\n",
    "print(f\"Sample text length: {metadata['size']} characters\")\n",
    "print(f\"\\n{SAMPLE_TEXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create a connection to your Neo4j database using the `Neo4jConnection` utility class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Clear Existing Data (Optional)\n",
    "\n",
    "For a clean start, remove any existing Document and Chunk nodes from previous runs using the utility method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j.clear_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Create Document Node\n",
    "\n",
    "First, create a Document node to represent the source file. This node stores metadata about where the content came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(driver, path: str, page: int) -> str:\n",
    "    \"\"\"Create a Document node and return its element ID.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            CREATE (d:Document {path: $path, page: $page})\n",
    "            RETURN elementId(d) as doc_id\n",
    "        \"\"\", path=path, page=page)\n",
    "        return result.single()[\"doc_id\"]\n",
    "\n",
    "doc_id = create_document(driver, DOCUMENT_PATH, DOCUMENT_PAGE)\n",
    "print(f\"Created Document node with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Split Text into Chunks\n",
    "\n",
    "Use `FixedSizeSplitter` from neo4j-graphrag-python to split the text into chunks with configurable size and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text using the utility function\n",
    "chunks = split_text(SAMPLE_TEXT, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} chars\")\n",
    "    print(f\"  {chunk[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Create Chunk Nodes\n",
    "\n",
    "Create Chunk nodes for each piece of text and link them to the Document with `FROM_DOCUMENT` relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(driver, doc_id: str, chunks: list[str]) -> list[str]:\n",
    "    \"\"\"Create Chunk nodes linked to a Document. Returns chunk element IDs.\"\"\"\n",
    "    chunk_ids = []\n",
    "    with driver.session() as session:\n",
    "        for index, text in enumerate(chunks):\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (d:Document) WHERE elementId(d) = $doc_id\n",
    "                CREATE (c:Chunk {text: $text, index: $index})\n",
    "                CREATE (c)-[:FROM_DOCUMENT]->(d)\n",
    "                RETURN elementId(c) as chunk_id\n",
    "            \"\"\", doc_id=doc_id, text=text, index=index)\n",
    "            chunk_id = result.single()[\"chunk_id\"]\n",
    "            chunk_ids.append(chunk_id)\n",
    "            print(f\"Created Chunk {index}\")\n",
    "    return chunk_ids\n",
    "\n",
    "chunk_ids = create_chunks(driver, doc_id, chunks)\n",
    "print(f\"\\nCreated {len(chunk_ids)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Link Chunks with NEXT_CHUNK\n",
    "\n",
    "Create `NEXT_CHUNK` relationships between sequential chunks. This preserves the original document order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_chunks(driver, chunk_ids: list[str]):\n",
    "    \"\"\"Create NEXT_CHUNK relationships between sequential chunks.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        for i in range(len(chunk_ids) - 1):\n",
    "            session.run(\"\"\"\n",
    "                MATCH (c1:Chunk) WHERE elementId(c1) = $id1\n",
    "                MATCH (c2:Chunk) WHERE elementId(c2) = $id2\n",
    "                CREATE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "            \"\"\", id1=chunk_ids[i], id2=chunk_ids[i+1])\n",
    "        print(f\"Created {len(chunk_ids) - 1} NEXT_CHUNK relationships\")\n",
    "\n",
    "link_chunks(driver, chunk_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Verify the Graph Structure\n",
    "\n",
    "Query the graph to see what we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_structure(driver):\n",
    "    \"\"\"Display the Document-Chunk graph structure.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Count nodes\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (d:Document)\n",
    "            OPTIONAL MATCH (d)<-[:FROM_DOCUMENT]-(c:Chunk)\n",
    "            RETURN d.path as document, d.page as page, count(c) as chunks\n",
    "        \"\"\")\n",
    "        print(\"=== Graph Structure ===\")\n",
    "        for record in result:\n",
    "            print(f\"Document: {record['document']} (page {record['page']})\")\n",
    "            print(f\"  Chunks: {record['chunks']}\")\n",
    "        \n",
    "        # Show chunk chain\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            OPTIONAL MATCH (c)-[:NEXT_CHUNK]->(next:Chunk)\n",
    "            RETURN c.index as idx, \n",
    "                   c.text as text,\n",
    "                   next.index as next_idx\n",
    "            ORDER BY c.index\n",
    "        \"\"\")\n",
    "        print(\"\\n=== Chunk Chain ===\")\n",
    "        for record in result:\n",
    "            next_str = f\" -> Chunk {record['next_idx']}\" if record['next_idx'] is not None else \" (end)\"\n",
    "            print(f\"Chunk {record['idx']}: \\\"{record['text']}\\\"{next_str}\")\n",
    "\n",
    "show_graph_structure(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Document-Chunk structure** - Documents are split into chunks for efficient retrieval\n",
    "2. **FROM_DOCUMENT relationship** - Links chunks back to their source document\n",
    "3. **NEXT_CHUNK relationship** - Preserves the sequential order of chunks\n",
    "\n",
    "This basic structure is the foundation for GraphRAG applications. In the next notebooks, you'll learn to:\n",
    "- Add **embeddings** to chunks for semantic search (01_02)\n",
    "- Extract **entities** from chunks to build a knowledge graph (01_03)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [Embeddings and Vector Search](01_02_embeddings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j-azure-ai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
